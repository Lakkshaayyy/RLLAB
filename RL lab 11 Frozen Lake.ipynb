{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMAk5g/Zz2ZMDqPCNJKC2yT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lakkshaayyy/RLLAB/blob/main/RL%20lab%2011%20Frozen%20Lake.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0oyoMtywkql",
        "outputId": "03d36d0b-a788-40b0-b049-1d7f4443804d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n"
          ]
        }
      ],
      "source": [
        "pip install gym\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "# Create the FrozenLake environment\n",
        "env = gym.make('FrozenLake8x8-v1')\n",
        "\n",
        "num_episodes = 5000\n",
        "gamma = 0.99\n",
        "\n",
        "# Initialize Q-table\n",
        "num_states = env.observation_space.n\n",
        "num_actions = env.action_space.n\n",
        "Q = np.zeros((num_states, num_actions))\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        # Implement epsilon-greedy exploration\n",
        "        epsilon = 0.1  # You can adjust this exploration rate\n",
        "        if np.random.rand() < epsilon:\n",
        "            action = env.action_space.sample()  # Choose a random action\n",
        "        else:\n",
        "            action = np.argmax(Q[state])  # Choose the action with the highest Q-value\n",
        "\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "        # Update the Q-value using the Q-learning update rule\n",
        "        learning_rate = 0.1  # You can adjust this learning rate\n",
        "        Q[state][action] += learning_rate * (reward + gamma * np.max(Q[next_state]) - Q[state][action])\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "# Test the learned policy\n",
        "num_test_episodes = 100\n",
        "num_successes = 0\n",
        "\n",
        "for _ in range(num_test_episodes):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        action = np.argmax(Q[state])  # Choose the action with the highest Q-value\n",
        "        state, _, done, _ = env.step(action)\n",
        "\n",
        "        if state == env.observation_space.n - 1:\n",
        "            num_successes += 1\n",
        "\n",
        "# Calculate the success rate\n",
        "success_rate = num_successes / num_test_episodes\n",
        "print(f\"Success Rate: {success_rate}\")\n",
        "\n",
        "# Close the environment\n",
        "env.close()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1HGfAz1F3QMo",
        "outputId": "7f0af596-7281-485a-d997-ae4e5e856a07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success Rate: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "\n",
        "# Create the FrozenLake environment\n",
        "env = gym.make('FrozenLake-v1')  # You can use 'FrozenLake-v0' for the deterministic version\n",
        "\n",
        "# Reset the environment to its initial state\n",
        "state = env.reset()\n",
        "\n",
        "# Define some actions\n",
        "actions = {\n",
        "    0: 'Left',\n",
        "    1: 'Down',\n",
        "    2: 'Right',\n",
        "    3: 'Up'\n",
        "}\n",
        "\n",
        "done = False\n",
        "while not done:\n",
        "    # Render the current state (optional, for visualization)\n",
        "    env.render()\n",
        "\n",
        "    # Choose a random action (replace with your agent's policy)\n",
        "    action = env.action_space.sample()\n",
        "\n",
        "    # Perform the chosen action\n",
        "    new_state, reward, done, info = env.step(action)\n",
        "\n",
        "    # Print the result of the action\n",
        "    print(f\"Action: {actions[action]}, State: {state}, New State: {new_state}, Reward: {reward}\")\n",
        "\n",
        "    state = new_state\n",
        "\n",
        "# Close the environment\n",
        "env.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inBv9AWo3bOh",
        "outputId": "b41ad6cb-db93-4106-d656-e72cd5bf9396"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:49: DeprecationWarning: \u001b[33mWARN: You are calling render method, but you didn't specified the argument render_mode at environment initialization. To maintain backward compatibility, the environment will render in human mode.\n",
            "If you want to render in human mode, initialize the environment in this way: gym.make('EnvName', render_mode='human') and don't call the render method.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action: Down, State: 0, New State: 4, Reward: 0.0\n",
            "Action: Left, State: 4, New State: 0, Reward: 0.0\n",
            "Action: Up, State: 0, New State: 0, Reward: 0.0\n",
            "Action: Right, State: 0, New State: 1, Reward: 0.0\n",
            "Action: Down, State: 1, New State: 5, Reward: 0.0\n"
          ]
        }
      ]
    }
  ]
}